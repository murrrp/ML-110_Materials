{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObjDet-UserGenModel_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAsn--Pvvqu5"
      },
      "source": [
        "**Reference:** https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b  \n",
        "**Reference (card images):** https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10  \n",
        "\n",
        "Part A: Dog Detector\n",
        "\n",
        "Part B: Card Detector **(This is covered in the video, please do not not do this part and only do parts A and C)**\n",
        "\n",
        "Part C: Custom Object Detector\n",
        "\n",
        "# BEFORE YOU BEGIN:\n",
        "Make sure that inside the \"Colab Notebooks\" folder inside your Google Drive that you create a folder called \"ObjDetection-UserGeneratedModel\", and put this colab script inside this folder. The commands below depend on this file structure.\n",
        "\n",
        "When you are ready to move on to part C, rename your folder for part A to something like ObjectDetection-Dog, and create a new folder for part C with the same name \"ObjDetection-UserGeneratedModel\".\n",
        "\n",
        "Also note that the default files from the materials folder are for part A and do not need to be modified. In the video, you will notice that the files are modified to have the correct output for part B, make sure you also make the necessary edits for part C. To clarify, **you do not need to do part B,** so you will not be using the card images and annotations in the materials link.\n",
        "\n",
        "#**Step 1:**   \n",
        "Run with TensorFlow 1.x  \n",
        "Select GPU under menu Runtime - Change runtime type.  \n",
        "Activate GPU for hardware acceleration used in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snZZt_ZgvhmW"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDntbjaZxAlL"
      },
      "source": [
        "#**Step 2:**  \n",
        "Mount your Google drive so can access folders/files.\n",
        "Copy/paste the authorization key into the box to allow access."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6G-mo4Pxnei"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFRUqgJ6xsJb"
      },
      "source": [
        "#**Step 3:**  \n",
        "Change directory to your working folder and clone the TensorFlow models repository.  \n",
        "Will create a folder  'models' --> community, official, research, etc.  \n",
        "We'll be working mostly in the the models-->research folder and subs under it.  \n",
        "Once this step is complete, you definitely will not need to re-run it even if your session expires.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veXxjAjjyA9f"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/\n",
        "!git clone https://github.com/tensorflow/models.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAf7URpkzLgl"
      },
      "source": [
        "#**Step 4:**  \n",
        "Install needed tools:  \n",
        "protobuf-compiler, python-pil, python-lxml, python-tk, Cython  \n",
        "\n",
        "Also install the current version of tf-slim (1.1)  \n",
        "Do it inside the object_detection/builders directory so can find the tensorflow.contrib module during the model builder test step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujlP4lCDzajN"
      },
      "source": [
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install Cython\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/builders\n",
        "!pip install --upgrade tf-slim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh6RwenkLuK9"
      },
      "source": [
        "#**Step 5:**  \n",
        "Compile the model definition.  \n",
        "Need to be in the /models/research/ folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhkTGQBqMQf5"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei-0G7RZMuON"
      },
      "source": [
        "#**Step 6:**  \n",
        "Set the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTPFjPxMM5YR"
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/drive/My Drive/Colab Notebooks/ObjDetection-UserGeneratedModel/models/research/:/content/drive/Colab Notebooks/ObjDetection-UserGeneratedModel/models/research/slim'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDE4cmbeNWp4"
      },
      "source": [
        "#**Step 7:**  \n",
        "**Run this before every session to setup the environment.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXj0c_ykN1k3"
      },
      "source": [
        "#####ALWAYS NEED TO RERUN THIS ON EVERY SESSION RESTART!!!\n",
        "#Run in the slim directory\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/slim/\n",
        "!python setup.py build\n",
        "!python setup.py install\n",
        "\n",
        "#Now run it again in the research directory\n",
        "%cd ../\n",
        "\n",
        "!python setup.py build\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff3HJBAgUGvz"
      },
      "source": [
        "#**Step 8:**  \n",
        "Run a test script to confirm all the training tools have been installed and the environment is setup properly. \n",
        "\n",
        "This script should run and output no errors if the environment is setup.\n",
        "\n",
        "Need to be in the /object_detection/builders directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A22-DUdbUX6P"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/builders/\n",
        "!python model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFMmqy0fZefm"
      },
      "source": [
        "#**Step 9:**  \n",
        "Setup the annotated images files for the training and testing.  \n",
        "\n",
        "**Make sure to check the resolution of your images.  Dog example was 640 x 480  RGB**   \n",
        "**Used higher res in the cards example, but need to make sure the annotation is on the resized image!!**  \n",
        "\n",
        "\n",
        "Create 'images' folder under object_detection and subfolders 'test' and 'train' under 'images' (case sensitive).  \n",
        "Copy your raw jpg images and annotation xml files into these folders, e.g. file_001-040.jpg and .xml into train and file_041-050.jpg and .xml into test.  **You will want approximately 20% of the images and annotations in the test folder and 80% in the train folder.**\n",
        "\n",
        "**For Part C: Read these steps carefully so that you won't have to repeat any steps.** \n",
        "\n",
        "1) Find a common household object and take around 55 pictures of it using your phone with varying backgrounds and distances and with multiple in the same picture (if possible). Try to take these pictures landscape. The first 50 pictures will be annotated and used for the test and train folders for this step similarly to part A. The last 5 pictures can be used for Step 19 when you actually test your model, so feel free not to annotate those 5.\n",
        "\n",
        "2) Resize these pictures to 640x480. See below for how to resize. This will speed up the training process since the pictures taken by your phone will be too high quality and the script actually will not work if you do not resize the images first. If some of your pictures are taken portrait style, rotate the images 90 degrees so that they are horizontal **before resizing**.\n",
        "\n",
        "Use the following script to resize if needed.  \n",
        "import glob  \n",
        "from PIL import Image  \n",
        "import os  \n",
        "class_name = 'CLASSNAME'  \n",
        "os.mkdir('resized')  \n",
        "file_list = sorted(glob.glob('*.jpg'))  \n",
        "for idx,file_name in enumerate(file_list):  \n",
        "  im = Image.open(file_name)  \n",
        "  new_width  = 640  \n",
        "  new_height = 480  \n",
        "  im = im.resize((new_width, new_height), Image.ANTIALIAS)  \n",
        "  im.save('resized/' + class_name + '_' + str(idx+1).zfill(3) + '.jpg')\n",
        "\n",
        "**Alternatively, use the following link:** https://bulkresizephotos.com/en\n",
        "\n",
        "3) Use the annotator under the ML110 materials link to annotate the resized images. Don't use the \"default\" label, make sure to create a label with the name of the object and use that label to label the objects. \n",
        "\n",
        "4) Now that you have both the images and annotations, follow the same steps for part A. \n",
        "\n",
        "Run the following script to convert between xml and csv.  \n",
        "The output csv files will be stored in /object_detection/data  called test_labels.csv and train_labels.csv. After running this step, locate these files and open them up to make sure that you have done this step properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKmv7pMTc4Sa"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    print('path=', path)\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        print('xml_file=', xml_file)\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            print('member=', member)\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(float(member[4][0].text)),\n",
        "                     int(float(member[4][1].text)),\n",
        "                     int(float(member[4][2].text)),\n",
        "                     int(float(member[4][3].text))\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    print('xml_list= ', xml_list)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "def main(directory_list):\n",
        "    for Image_cat in directory_list:\n",
        "        image_path = os.path.join(os.getcwd(), 'images/{}'.format(Image_cat))\n",
        "        print('image_path= ', image_path)\n",
        "        xml_df = xml_to_csv(image_path) \n",
        "        xml_df.to_csv('data/{}_labels.csv'.format(Image_cat), index=None)\n",
        "        print('Successfully converted xml to csv.')\n",
        "main(['train','test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBoA8XiLieqt"
      },
      "source": [
        "#**Step 10:**  \n",
        "Upload the \"generate_tfrecord.py\" file from the materials folder to the /models/research/object_detection directory.  All the files are already setup for part A and part C, so no further modifications to the file are needed. In the video, the correct file is shown for part B, the card example.\n",
        "\n",
        "Now generate the training and testing record files.\n",
        "Run the script each for training and testing.  \n",
        "\n",
        "python generate_tfrecord.py --label='LABEL' --csv_input=data/train_labels.csv --output_path=data/train.record --img_path=images/train  \n",
        "python generate_tfrecord.py --label='LABEL' --csv_input=data/test_labels.csv --output_path=data/test.record --img_path=images/test  \n",
        "\n",
        "Where LABEL is the label from the csv file (class column in the file). e.g. LABEL=='dog' in the dog example.  \n",
        "\n",
        "If this step was run correctly, you should have two new files under data: \"test.record\" and \"train.record\".\n",
        "\n",
        "**For part C:**\n",
        "\n",
        "Modify  the label command line arguments\n",
        "--label1='LABEL1' with the name of your custom object.\n",
        "\n",
        "Need to be in the object_detection directory.  \n",
        "\n",
        "\n",
        "If get error ModuleNotFoundError: No module named 'object_detection' it's likely because the session expired and was restarted.  In that case need to rerun step 7. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN77XEeRkiZH"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "#### ****\n",
        "#### NOTE NEED TO CHANGE LABEL TO EACH OF YOUR LABELS IN THE FILES: train_labels.csv and test_labels.csv\n",
        "#### ****\n",
        "#!python generate_tfrecord.py --label='LABEL' --csv_input=data/train_labels.csv --output_path=data/train.record --img_path=images/train\n",
        "#!python generate_tfrecord.py --label='LABEL' --csv_input=data/test_labels.csv --output_path=data/test.record --img_path=images/test\n",
        "\n",
        "!python generate_tfrecord.py --label1='dog' --csv_input=data/train_labels.csv --output_path=data/train.record --img_path=images/train\n",
        "!python generate_tfrecord.py --label1='dog' --csv_input=data/test_labels.csv --output_path=data/test.record --img_path=images/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzO4h0yIkqds"
      },
      "source": [
        "#**Step 11:**  \n",
        "Get the pre-trained object detection model from TensorFlow and decompress it.    \n",
        "Need to be in the /object_detection/ directory.  Both the tar file and the extracted directory will be stored there.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiLXwlQoq4Pv"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz\n",
        "!tar -xvf ssd_mobilenet_v1_coco_11_06_2017.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdZ8lz7vtaP6"
      },
      "source": [
        "#**Step 12:**  \n",
        "Create a directory under object_detection called 'training'.  \n",
        "Upload the ssd_mobilenet_v1_coco.config file from the materials folder and store it in /training.  \n",
        "\n",
        "For Part B this file was modified because there was more than 1 class in the model, but for Part C you should only have 1 object in your model so no modifications are necessary for both Part A and Part C.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLB2tEsn2QCk"
      },
      "source": [
        "#**Step 13:**\n",
        "Upload file 'object-detection.pbtxt' from the materials folder into the training directory.\n",
        "\n",
        "This file contains records of a mapping of the id's to the labels.  \n",
        "If you have more than one object you are training then need to have more than one record.  \n",
        "Follow the id mapping you assigned in the generate_tfrecord step.  \n",
        "Need to save to /object_detection/training/ directory.  \n",
        "At this point the training folder should only contain two files, in the video there appears to be many more files and these will be added in the next step.\n",
        "Modifications are only needed in part C, the video shows the correct modifications needed for part B.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBuo8EGnuftz"
      },
      "source": [
        "#**Step 14:**  \n",
        "Execute training.  (This is a long step, usually takes around 30 minutes.)\n",
        "\n",
        "Copy or move the train.py file from the 'legacy' folder into the object detection folder (move it up one level)\n",
        "This training will run indefinitely.  You can run for a fixed number of steps by adding num_steps in the ssd_mobilenet_v1_coco.config file. (just search for num_steps and uncomment the line).  \n",
        "Otherwise, stop the cell when loss is < 1.0. Don't worry if the training somehow gets interrupted or if you accidentally stop this block too early, the training will continue on the last model saved (the script saves a model every 1000 steps or so)\n",
        "\n",
        "Need to be in object_detection directory to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3GFcByeuvUx"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we4ksNvIuzKB"
      },
      "source": [
        "#**Step 15:**  \n",
        "Export the inference graph.  \n",
        "Create a folder called trained_inference_graph under the object_detection directory then run this script from the object_detection directory.  \n",
        "\n",
        "**Update model.ckpt-xxxx in the python command line to the highest number .ckpt file stored in the /training directory.**\n",
        "\n",
        "If get error ModuleNotFoundError: No module named 'object_detection' it's likely because the session expired and was restarted.  In that case need to rerun step 7.  \n",
        "Also if the session expired, make sure you have tensorflow 1.x active (step 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gb7UrLA5MGc"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory trained_inference_graph/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCqiFdzm5i8d"
      },
      "source": [
        "#**Step 16:**  \n",
        "Zip the inference graph.  \n",
        "Name the graph according to your object detection project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_sSYH9n5tyl"
      },
      "source": [
        "#### Edit PROJECT to your object detection project.\n",
        "!zip -r PROJECT.zip trained_inference_graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xhk70b76Ejq"
      },
      "source": [
        "#**Step 17:**  \n",
        "#Reinstall tf-slim (to 1.2) here...\n",
        "You'll have to install tf-slim-1.2 (was previously tf-slim-1.1.0)  \n",
        "!pip install git+https://github.com/google-research/tf-slim.git\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEZljF206JgP"
      },
      "source": [
        "!pip install git+https://github.com/google-research/tf-slim.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qo0vbT67J1U"
      },
      "source": [
        "#**Step 18:**  \n",
        "Find the folder called 'test_images' under object_detection.  \n",
        "Copy some images to test in the test_images folder and rename image1.jpg, image2.jpg, etc.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDlBfIsv7bcY"
      },
      "source": [
        "#**Step 19:**  \n",
        "Test the model by uploading images into the test_images folder, naming them image1.jpg, image2.jpg, etc.\n",
        "\n",
        "Update the number of images to test in the code by modifying the range for the variable TEST_IMAGE_PATHS\n",
        "\n",
        "Need to be in the /object_detection/ directory.  \n",
        "You should see the images output with bounding boxes.  \n",
        "\n",
        "If you've restarted your session:  \n",
        "If you receive tf-slim specific errors (e.g. can't find tf-slim), run step 17.  \n",
        "If you receive tensorflow specific errors (e.g. tensorflow has no module GraphDef) run step 1 (note, may also have to restart runtime from the main menu if get message that tensorflow is already loaded at version 2.x)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBD8la007uOZ"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from distutils.version import StrictVersion\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "\n",
        "### Model preparation variable\n",
        "MODEL_NAME = 'trained_inference_graph'\n",
        "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
        "PATH_TO_LABELS = 'training/object-detection.pbtxt'\n",
        "NUM_CLASSES = 1 #remember number of objects you are training? cool.\n",
        "\n",
        "\n",
        "### Load a (frozen) Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "###Loading label map\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Load image into numpy function\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Path to the test images folder\n",
        "### UPDATE THE RANGE TO THE NUMBER OF IMAGES IN THE FOLDER\n",
        "PATH_TO_TEST_IMAGES_DIR = 'test_images/'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 10) ]\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Function to run inference on a single image which will later be used in an iteration\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: image})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.int64)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "\n",
        "\n",
        "### To iterate on each image in the test image path defined \n",
        "### NB define the range of numbers and let it match the number of imAGES IN TEST FOLDER +1\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=1)\n",
        "  display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmX8VDzqHoy4"
      },
      "source": [
        "#**Step 20**  \n",
        "\n",
        "Next, we'll need to convert the TensorFlow model to TFLite so we can run it on the Pi.\n",
        "\n",
        "Make a new directory called TFLite_model and create the TFLite inference graph. Update model.ckpt-xxxx in the python command line to the highest number .ckpt file stored in the /train or /training directory, depending on where the model is stored. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKVa-jHNKLo7"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "\n",
        "!mkdir TFLite_model\n",
        "!python export_tflite_ssd_graph.py --pipeline_config_path=training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix=training/model.ckpt-XXXX --output_directory=TFLite_model --add_postprocessing_op=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHdACOo3LddX"
      },
      "source": [
        "#**Step 21**  \n",
        "Install TensorFlow Lite Optimizing Converter (TOCO), and run the model through TOCO to get a model that can be used by TensorFlow Lite.  \n",
        "\n",
        "Make sure that your graph def file points to the **tflite_graph.pb** file, **not** the frozen_inference_graph.pb file from the previously exported model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nXTnaS-LywS"
      },
      "source": [
        "!pip install toco\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/ObjDetection-UserGeneratedModel/models/research/object_detection/\n",
        "!toco --graph_def_file=TFLite_model/tflite_graph.pb --output_file=TFLite_model/detect.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=normalized_input_image_tensor --output_array='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --input_type=FLOAT --allow_custom_ops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nbzWEI_O4tT"
      },
      "source": [
        "#**Step 22:**\n",
        "Create a TFLite compatible label map\n",
        "\n",
        "TFLite's labelmaps are organized differently than that of TensorFlow. Instead of explicitly stating the name and ID, TFLite just lists each class.\n",
        "\n",
        "Thus, we need to create a new label map that matches the TensorFlow Lite style. Open a text editor and list each class in order of their class number.  Because parts A and C only have 1 class trained on, simply type in the name of your class.\n",
        "Then, save the file as “labelmap.txt” in the TFLite_model folder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfXqdI1zPzsD"
      },
      "source": [
        "#**Step 23:**\n",
        "Move the TFLite_model folder over to your Pi and run the model\n",
        "\n",
        "Download and zip your TFLite_model folder to your computer. \n",
        "\n",
        "\n",
        "As an alternative to copying your files over via thumbdrive, you can use SCP to copy it over.  \n",
        "\n",
        "'scp myfile.txt pi@192.168.1.3:tflite1/'\n",
        "\n",
        "This should prompt you to enter the Pi's password. Enter it, then it should show a progress percentage.\n",
        "\n",
        "Access your Pi (use VNC viewer if you are operating your Pi remotely), navigate to the tflite1 folder.\n",
        "\n",
        "Run `source tflite1-env/bin/activate` to activate the computer vision environment, and then run `python3 TFLite_detection_webcam.py --modeldir=TFLite_model/` to test your model.\n"
      ]
    }
  ]
}